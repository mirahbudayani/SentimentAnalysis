{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9a37bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/fooenglow/naive-bayes-from-scratch-sentiment-analysis/notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import nltk\n",
    "import string, re\n",
    "import gc # garbage collector to manage RAM usage\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667c6368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Adiiiinnnnnn Harusnya adik2 juga mengapresias...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Siapa yang bangun tidur masih terharu sekarang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Still astonished and thrilled about the Indone...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nyambeksyariah harus lawan pake UU TPKS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saksikan Perjuangan DPR RI bersama Rakyat untu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0  @Adiiiinnnnnn Harusnya adik2 juga mengapresias...          0\n",
       "1  Siapa yang bangun tidur masih terharu sekarang...          1\n",
       "2  Still astonished and thrilled about the Indone...          1\n",
       "3           @nyambeksyariah harus lawan pake UU TPKS          1\n",
       "4  Saksikan Perjuangan DPR RI bersama Rakyat untu...          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"C:/Users/mirah/OneDrive/TA/DATAWITHSENTIMENT.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b49890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "#cleansing data\n",
    "def cleansing(data):\n",
    "    #hapus https\n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    data = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", data)\n",
    "    data = re.sub(r'\\d+', '', data)\n",
    "    \n",
    "    # hapus punctuation\n",
    "    remove = string.punctuation\n",
    "    translator = str.maketrans(remove, ' '*len(remove))\n",
    "    data = data.translate(translator)\n",
    "    \n",
    "    # remove ASCII dan unicode\n",
    "    data = data.encode('ascii', 'ignore').decode('utf-8')\n",
    "    data = re.sub(r'[^\\x00-\\x7f]',r'', data)\n",
    "    \n",
    "    # remove newline\n",
    "    data = data.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    \n",
    "    #hapus whitespace\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    data = re.sub(pattern, ' ', data)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
    "    data = data.replace('?', ' ? ').replace(')', ') ')\n",
    "    \n",
    "    # # this is a docstring\n",
    "    \"\"\"\n",
    "    The function will remove accented characters from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : Málaga, àéêöhello\n",
    "    Output : Malaga, aeeohello    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    data = unidecode.unidecode(data)\n",
    "    \n",
    "    return data\n",
    "# jalankan cleansing data\n",
    "review = []\n",
    "for index, row in tweets.iterrows():\n",
    "    review.append(cleansing(row[\"Tweet\"]))\n",
    "    \n",
    "tweets[\"Tweet\"] = review\n",
    "# remove consecutive duplicates from string\n",
    "def remove_consec_duplicates(s):\n",
    "    new_s = \"\"\n",
    "    prev = \"\"\n",
    "    for c in s:\n",
    "        if len(new_s) == 0:\n",
    "            new_s += c\n",
    "            prev = c\n",
    "        if c == prev:\n",
    "            continue\n",
    "        else:\n",
    "            new_s += c\n",
    "            prev = c\n",
    "    return new_s\n",
    "review = []\n",
    "for index, row in tweets.iterrows():\n",
    "    review.append(remove_consec_duplicates(row[\"Tweet\"]))\n",
    "    \n",
    "tweets[\"Tweet\"] = review\n",
    "\n",
    "def cleansing(data):\n",
    "    # lower text\n",
    "    data = data.lower()\n",
    "    return data\n",
    "# jalankan cleansing data\n",
    "review = []\n",
    "for index, row in tweets.iterrows():\n",
    "    review.append(cleansing(row[\"Tweet\"]))\n",
    "    \n",
    "tweets[\"Tweet\"] = review\n",
    "# import library\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    " \n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    " \n",
    "# Contoh\n",
    "kalimat = 'Dengan Menggunakan Python dan Library Sastrawi saya dapat melakukan proses Stopword Removal'\n",
    "stop = stopword.remove(kalimat)\n",
    "#print(stop)\n",
    "\n",
    "# lakukan pada data kita\n",
    "\n",
    "review = []\n",
    "for index, row in tweets.iterrows():\n",
    "    review.append(stopword.remove(row[\"Tweet\"]))\n",
    "    \n",
    "tweets[\"Tweet\"] = review\n",
    "\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemover, ArrayDictionary\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()\n",
    "# Ambil Stopword bawaan\n",
    "stop_factory = StopWordRemoverFactory().get_stop_words()\n",
    "more_stopword = [\"apa\", \"tau\",\"mjd\", \"sbg\", \"dgn\", \"dg\", \"loh\", \"lho\", \"the\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
    "\n",
    "# Merge stopword\n",
    "data = stop_factory + more_stopword\n",
    "\n",
    "dictionary = ArrayDictionary(data)\n",
    "str = StopWordRemover(dictionary)\n",
    "review = []\n",
    "for index, row in tweets.iterrows():\n",
    "    review.append(str.remove(row[\"Tweet\"]))\n",
    "    \n",
    "tweets[\"Tweet\"] = review\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# contoh\n",
    "kalimat = 'Liverpool adalah klub hebat tidak seperti si itu WkwkWK'\n",
    "katadasar = stemmer.stem(kalimat)\n",
    " \n",
    "#print(katadasar)\n",
    "\n",
    "# implementasi pada data kita\n",
    "review = []\n",
    "for index, row in tweets.iterrows():\n",
    "    review.append(stemmer.stem(row[\"Tweet\"]))\n",
    "    \n",
    "tweets[\"Tweet\"] = review\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def cleansing(data):\n",
    "    tokenizer=word_tokenize()\n",
    "    return data\n",
    "review = []\n",
    "for index, row in tweets.iterrows():\n",
    "    review.append(word_tokenize(row[\"Tweet\"]))\n",
    "    \n",
    "tweets[\"Tweet\"] = review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa14ee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[adin, harus, adik, apresiasi, disahkanya, utp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[siapa, bangun, tidur, haru, sekarang, nyebut,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[stil, astonished, thriled, the, indonesian, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nyambeksyariah, lawan, pake, tpks]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[saksi, juang, dpr, sama, rakyat, tpks, sama, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0  [adin, harus, adik, apresiasi, disahkanya, utp...          0\n",
       "1  [siapa, bangun, tidur, haru, sekarang, nyebut,...          1\n",
       "2  [stil, astonished, thriled, the, indonesian, p...          1\n",
       "3                [nyambeksyariah, lawan, pake, tpks]          1\n",
       "4  [saksi, juang, dpr, sama, rakyat, tpks, sama, ...          1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d4aded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4388\n",
      "1098\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[jalan, liku, tpks, sah, jadi, butuh, waktu, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[mardiasih, kalo, kasus, selangkanganya, gesek...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[puan, sangat, rasa, banga, banyak, perempuan,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tuju, nih, moga, april, sah, perempuanbisa, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tpks, beri, lindung, payung, hukum, yg, lebih...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0  [jalan, liku, tpks, sah, jadi, butuh, waktu, t...          1\n",
       "1  [mardiasih, kalo, kasus, selangkanganya, gesek...          1\n",
       "2  [puan, sangat, rasa, banga, banyak, perempuan,...          1\n",
       "3  [tuju, nih, moga, april, sah, perempuanbisa, p...          1\n",
       "4  [tpks, beri, lindung, payung, hukum, yg, lebih...          1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets['Tweet'], tweets['Sentiment'], test_size=0.2, \n",
    "                                                    stratify=tweets['Sentiment'], random_state=1)\n",
    "\n",
    "# Combine the results into train and test dataframe\n",
    "tweets_train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "tweets_test = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "\n",
    "print(len(tweets_train))\n",
    "print(len(tweets_test))\n",
    "tweets_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f51666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabularies in the training set is 6014\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "\n",
    "for index, value in tweets_train['Tweet'].items():\n",
    "    for word in value:\n",
    "        vocab.append(word)\n",
    "        \n",
    "# Transform vocab from list to set to remove duplicate words\n",
    "vocab = list(set(vocab))\n",
    "\n",
    "print(\"Number of vocabularies in the training set is {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e78825dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debat</th>\n",
       "      <th>today</th>\n",
       "      <th>catanthr</th>\n",
       "      <th>dhukum</th>\n",
       "      <th>jijik</th>\n",
       "      <th>anshori</th>\n",
       "      <th>wawancara</th>\n",
       "      <th>sabriana</th>\n",
       "      <th>anget</th>\n",
       "      <th>silam</th>\n",
       "      <th>...</th>\n",
       "      <th>jaksa</th>\n",
       "      <th>listyosigitp</th>\n",
       "      <th>years</th>\n",
       "      <th>kala</th>\n",
       "      <th>dikat</th>\n",
       "      <th>hebatcbanget</th>\n",
       "      <th>eks</th>\n",
       "      <th>gmni</th>\n",
       "      <th>mmang</th>\n",
       "      <th>vidio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6014 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   debat  today  catanthr  dhukum  jijik  anshori  wawancara  sabriana  anget  \\\n",
       "0      0      0         0       0      0        0          0         0      0   \n",
       "1      0      0         0       0      0        0          0         0      0   \n",
       "2      0      0         0       0      0        0          0         0      0   \n",
       "3      0      0         0       0      0        0          0         0      0   \n",
       "4      0      0         0       0      0        0          0         0      0   \n",
       "\n",
       "   silam  ...  jaksa  listyosigitp  years  kala  dikat  hebatcbanget  eks  \\\n",
       "0      0  ...      0             0      0     0      0             0    0   \n",
       "1      0  ...      0             0      0     0      0             0    0   \n",
       "2      0  ...      0             0      0     0      0             0    0   \n",
       "3      0  ...      0             0      0     0      0             0    0   \n",
       "4      0  ...      0             0      0     0      0             0    0   \n",
       "\n",
       "   gmni  mmang  vidio  \n",
       "0     0      0      0  \n",
       "1     0      0      0  \n",
       "2     0      0      0  \n",
       "3     0      0      0  \n",
       "4     0      0      0  \n",
       "\n",
       "[5 rows x 6014 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For loops implementation - Did not run this step as it will take very long time to run\n",
    "vocab_count_per_tweet = {word: [0] * len(tweets_train) for word in vocab}\n",
    "\n",
    "for index, value in tweets_train['Tweet'].items():\n",
    "    for word in value:\n",
    "        vocab_count_per_tweet[word][index] += 1\n",
    "        \n",
    "vocab_counts = pd.DataFrame(vocab_count_per_tweet)\n",
    "vocab_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5143aec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4388x6004 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 57599 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Combine all the tweets into a single list\n",
    "corpus = tweets_train['Tweet'].apply(lambda x:' '.join(x))\n",
    "\n",
    "# Fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_wc = vectorizer.fit_transform(corpus)\n",
    "X_train_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee88279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (4388, 6004)\n",
      "Shape of y_train: (4388,)\n",
      "Shape of X_test: (1098, 6004)\n",
      "Shape of y_test: (1098,)\n"
     ]
    }
   ],
   "source": [
    "# Convert y_train and y_test to array\n",
    "y_train = np.array(tweets_train['Sentiment'])\n",
    "y_test = np.array(tweets_test['Sentiment'])\n",
    "\n",
    "# Create X_test_wc using sklearn CountVectorizer\n",
    "corpus = tweets_test['Tweet'].apply(lambda x:' '.join(x))\n",
    "X_test_wc = vectorizer.transform(corpus)\n",
    "\n",
    "# Ensure the shape of X, y in training and test set is correct\n",
    "print(\"Shape of X_train: {}\".format(X_train_wc.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print(\"Shape of X_test: {}\".format(X_test_wc.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bc8e6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9080145719489982"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB() # default smoothing parameter alpha=1\n",
    "clf.fit(X_train_wc, y_train)\n",
    "clf.score(X_test_wc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ec3fdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 39,  40],\n",
       "       [ 61, 958]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate confusion matrix plot\n",
    "y_pred = clf.predict(X_test_wc)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a5a778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: \n",
      "0.9080145719489982\n",
      "precision score: \n",
      "0.9599198396793587\n",
      "recall score: \n",
      "0.9401373895976447\n",
      "f1 score: \n",
      "0.9499256321269212\n",
      "[[ 39  40]\n",
      " [ 61 958]]\n"
     ]
    }
   ],
   "source": [
    "# import library evaluation\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score\n",
    "\n",
    "# accuracy score\n",
    "print(\"accuracy score: \")\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "# precision score\n",
    "print(\"precision score: \")\n",
    "print(precision_score(y_test, y_pred, pos_label=1))\n",
    "\n",
    "# recall score\n",
    "print(\"recall score: \")\n",
    "print(recall_score(y_test, y_pred, pos_label=1))\n",
    "# f1 score\n",
    "print(\"f1 score: \")\n",
    "print(f1_score(y_test, y_pred, pos_label=1))\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa53e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
